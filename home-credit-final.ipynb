{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-13T18:13:52.317571Z",
     "iopub.status.busy": "2022-12-13T18:13:52.317114Z",
     "iopub.status.idle": "2022-12-13T18:13:52.349606Z",
     "shell.execute_reply": "2022-12-13T18:13:52.348465Z",
     "shell.execute_reply.started": "2022-12-13T18:13:52.317480Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function's\n",
    "    These are all the funtions used in the model preparation and model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:13:52.353289Z",
     "iopub.status.busy": "2022-12-13T18:13:52.352100Z",
     "iopub.status.idle": "2022-12-13T18:13:52.383644Z",
     "shell.execute_reply": "2022-12-13T18:13:52.382201Z",
     "shell.execute_reply.started": "2022-12-13T18:13:52.353241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "def checking_missing_values(data):\n",
    "    count = data.isnull().sum().sort_values(ascending=False)\n",
    "    percentage = ((data.isnull().sum()/len(data)*100)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([count, percentage], axis=1, keys=['Count','Percentage'])\n",
    "    return missing_data\n",
    "\n",
    "# Dropping the Missing values if greater than 40%\n",
    "def dropping_missing_values(data):\n",
    "    percentage = 40.0\n",
    "    min_count = int(((100-percentage)/100)*data.shape[0] + 1)\n",
    "    dropped_data = data.dropna(axis = 1, thresh = min_count)\n",
    "    return dropped_data\n",
    "\n",
    "# Checking for Duplicate data\n",
    "def checking_duplicate_data(data):\n",
    "    columns = []\n",
    "    for col in data.columns:\n",
    "        if col!='SK_ID_CURR':\n",
    "            columns.append(col)\n",
    "    flag = data[data.duplicated(subset = columns, keep=False)]\n",
    "    return flag\n",
    "\n",
    "# Imputing categorical missing values with mode\n",
    "def imputing_categorical_missing_values(data):\n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        if col_type == object:\n",
    "            data[col] = data[col].fillna(data[col].mode().iloc[0])\n",
    "    return data\n",
    "\n",
    "# One Hot Encoding the categorical variables\n",
    "def encoding_categorical(data):\n",
    "    # Dropping the first column to not get into dummy variable trap\n",
    "    data = pd.get_dummies(data, drop_first = True)\n",
    "    return data\n",
    "\n",
    "# Mean Imputation To impute the missing values in numerical columns\n",
    "def mean_imputation(data):\n",
    "    mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    data = pd.DataFrame(mean_imputer.fit_transform(data), columns=data.columns)\n",
    "    return data\n",
    "\n",
    "# MICE Imputation :-> To impute the missing values in numerical columns\n",
    "def mice_imputation(data):\n",
    "    mice_imputer = IterativeImputer(estimator=linear_model.BayesianRidge())\n",
    "    df_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(data), columns=data.columns)\n",
    "    return df_mice_imputed\n",
    "\n",
    "def IV_calculation(X, y, head = 100):\n",
    "    clf = WOE()\n",
    "    clf.fit(X, y)\n",
    "    iv_values = clf.iv_df.head(head)\n",
    "    return iv_values\n",
    "\n",
    "def tuning_splits(X, y):\n",
    "    n_splits = [5, 6, 7, 8, 9,10]\n",
    "    scores = []\n",
    "    for s in n_splits:\n",
    "        kf = StratifiedKFold(n_splits=s, shuffle=True, random_state=42)\n",
    "        score = cross_val_score(lm.LogisticRegression(), X, y, cv= kf, scoring=\"roc_auc\")\n",
    "        scores.append(np.mean(score))\n",
    "    return scores\n",
    "\n",
    "def tuning_C(X, y):\n",
    "    C = [0.001, 0.01, 0.1, 1, 10]\n",
    "    scores = []\n",
    "    kf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    for c in C:\n",
    "        score = cross_val_score(lm.LogisticRegression(C = c, random_state = 42), X, y, cv= kf, scoring=\"roc_auc\")\n",
    "        scores.append(np.mean(score))\n",
    "    return scores\n",
    "\n",
    "def tuning_solver(X, y):\n",
    "    algo = ['lbfgs', 'liblinear']\n",
    "    scores = []\n",
    "    kf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    for alg in algo:\n",
    "        score = cross_val_score(lm.LogisticRegression(max_iter = 3000, solver = alg, random_state = 42), X, y, cv= kf, scoring=\"roc_auc\")\n",
    "        scores.append(np.mean(score))\n",
    "    return scores\n",
    "    \n",
    "\n",
    "def model(X, y, tuning = False):\n",
    "    if tuning == False:\n",
    "        logreg = LogisticRegression()\n",
    "    else:\n",
    "        logreg = LogisticRegression(C=0.01, class_weight = 'balanced', max_iter=200, solver = '')\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "    cv_scores = cross_validate(logreg, X, Y, n_jobs=-1, cv=kf, scoring ='roc_auc')\n",
    "    \n",
    "    return cv_scores, logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the required Libraries\n",
    "    Installing the libraries for calculating Information Value and Cramer's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:13:52.385896Z",
     "iopub.status.busy": "2022-12-13T18:13:52.385136Z",
     "iopub.status.idle": "2022-12-13T18:14:18.036896Z",
     "shell.execute_reply": "2022-12-13T18:14:18.035412Z",
     "shell.execute_reply.started": "2022-12-13T18:13:52.385858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xverse\n",
      "  Downloading xverse-1.0.5-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in e:\\anaconda\\lib\\site-packages (from xverse) (3.4.3)\n",
      "Requirement already satisfied: pandas>=0.21.1 in e:\\anaconda\\lib\\site-packages (from xverse) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.11.3 in e:\\anaconda\\lib\\site-packages (from xverse) (1.20.3)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in e:\\anaconda\\lib\\site-packages (from xverse) (0.12.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in e:\\anaconda\\lib\\site-packages (from xverse) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in e:\\anaconda\\lib\\site-packages (from xverse) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in e:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in e:\\anaconda\\lib\\site-packages (from matplotlib>=3.0.3->xverse) (8.4.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in e:\\anaconda\\lib\\site-packages (from pandas>=0.21.1->xverse) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\anaconda\\lib\\site-packages (from scikit-learn>=0.19.0->xverse) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in e:\\anaconda\\lib\\site-packages (from scikit-learn>=0.19.0->xverse) (1.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in e:\\anaconda\\lib\\site-packages (from statsmodels>=0.6.1->xverse) (0.5.2)\n",
      "Requirement already satisfied: six in e:\\anaconda\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.3->xverse) (1.16.0)\n",
      "Installing collected packages: xverse\n",
      "Successfully installed xverse-1.0.5\n",
      "Collecting association-metrics\n",
      "  Downloading association-metrics-0.0.1.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: association-metrics\n",
      "  Building wheel for association-metrics (setup.py): started\n",
      "  Building wheel for association-metrics (setup.py): finished with status 'done'\n",
      "  Created wheel for association-metrics: filename=association_metrics-0.0.1-py3-none-any.whl size=3952 sha256=b83813a617834b29bfab1ef28bf2bf4f9636841203dd0c3a2c6338038b7b8fdf\n",
      "  Stored in directory: c:\\users\\vset info\\appdata\\local\\pip\\cache\\wheels\\6c\\0d\\73\\a61ad5997c87d5c9fd4a0f21243b97b1ae8807cfbee4ebdd7d\n",
      "Successfully built association-metrics\n",
      "Installing collected packages: association-metrics\n",
      "Successfully installed association-metrics-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xverse\n",
    "!pip install association-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:18.041600Z",
     "iopub.status.busy": "2022-12-13T18:14:18.041026Z",
     "iopub.status.idle": "2022-12-13T18:14:18.914163Z",
     "shell.execute_reply": "2022-12-13T18:14:18.912991Z",
     "shell.execute_reply.started": "2022-12-13T18:14:18.041542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Removing the Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Importing the Logestic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# For calculation of Weight of Evidence and Information Value\n",
    "from xverse.transformer import WOE\n",
    "\n",
    "# Cramers V\n",
    "import association_metrics as am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:18.916350Z",
     "iopub.status.busy": "2022-12-13T18:14:18.915922Z",
     "iopub.status.idle": "2022-12-13T18:14:25.609191Z",
     "shell.execute_reply": "2022-12-13T18:14:25.608061Z",
     "shell.execute_reply.started": "2022-12-13T18:14:18.916305Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/home-credit-default-risk/application_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VSETIN~1\\AppData\\Local\\Temp/ipykernel_19296/3006027984.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing the Dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/kaggle/input/home-credit-default-risk/application_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The shape of data:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/home-credit-default-risk/application_train.csv'"
     ]
    }
   ],
   "source": [
    "# Importing the Dataset\n",
    "df = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\n",
    "print('The shape of data:',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Preprocessing\n",
    "    In this we gonna check for missing value's, dropping the missing value column's, checking for duplicate value's and treating the anomalies.\n",
    "    Also, we will impute the missing value's using Mean as well as MICE Imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:25.612153Z",
     "iopub.status.busy": "2022-12-13T18:14:25.611720Z",
     "iopub.status.idle": "2022-12-13T18:14:26.161368Z",
     "shell.execute_reply": "2022-12-13T18:14:26.160587Z",
     "shell.execute_reply.started": "2022-12-13T18:14:25.612111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.872297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.872297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.872297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.432963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.432963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Count  Percentage\n",
       "COMMONAREA_MEDI           214865   69.872297\n",
       "COMMONAREA_AVG            214865   69.872297\n",
       "COMMONAREA_MODE           214865   69.872297\n",
       "NONLIVINGAPARTMENTS_MODE  213514   69.432963\n",
       "NONLIVINGAPARTMENTS_AVG   213514   69.432963\n",
       "...                          ...         ...\n",
       "NAME_HOUSING_TYPE              0    0.000000\n",
       "NAME_FAMILY_STATUS             0    0.000000\n",
       "NAME_EDUCATION_TYPE            0    0.000000\n",
       "NAME_INCOME_TYPE               0    0.000000\n",
       "SK_ID_CURR                     0    0.000000\n",
       "\n",
       "[122 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Missing Values\n",
    "checking_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:26.163228Z",
     "iopub.status.busy": "2022-12-13T18:14:26.162886Z",
     "iopub.status.idle": "2022-12-13T18:14:26.542913Z",
     "shell.execute_reply": "2022-12-13T18:14:26.541685Z",
     "shell.execute_reply.started": "2022-12-13T18:14:26.163198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 73)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing those columns which are having more than 40% of missing values\n",
    "df = dropping_missing_values(df)\n",
    "\n",
    "# Shape of application after dropping the columns having more than 40% missing values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:26.544659Z",
     "iopub.status.busy": "2022-12-13T18:14:26.544305Z",
     "iopub.status.idle": "2022-12-13T18:14:28.837427Z",
     "shell.execute_reply": "2022-12-13T18:14:28.835660Z",
     "shell.execute_reply.started": "2022-12-13T18:14:26.544627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no. of duplicates in the data: 0\n",
      "\n",
      "Cleaning Started....\n",
      "\n",
      "Cleaning done!!!\n",
      "Imputing and Encoding Categorical Variables with Mode....\n",
      "\n",
      "Imputing and Encoding Done!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate data\n",
    "temp = checking_duplicate_data(df)\n",
    "print(f\"The no. of duplicates in the data: {temp.shape[0]}\\n\")\n",
    "\n",
    "print(f\"Cleaning Started....\\n\")\n",
    "# Cleaning columns as some columns contain XNA instead of NA and some are having typing mistake\n",
    "## Column -> CODE_GENDER\n",
    "df['CODE_GENDER'] = df['CODE_GENDER'].replace(\"XNA\", np.nan)\n",
    "## Column -> ORGANIZATION_TYPE\n",
    "df['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].replace(\"XNA\", \"Other\")\n",
    "## Column -> DAYS_EMPLOYED\n",
    "df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "print(f\"Cleaning done!!!\")\n",
    "\n",
    "# Getting application without ID and Target\n",
    "application_without_ID_Target = df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "\n",
    "print(\"Imputing and Encoding Categorical Variables with Mode....\\n\")\n",
    "\n",
    "# Imputing the categorical Values with Mode\n",
    "application_without_ID_Target = imputing_categorical_missing_values(application_without_ID_Target)\n",
    "\n",
    "# Encoding the Categorical Columns\n",
    "application_without_ID_Target = encoding_categorical(application_without_ID_Target)\n",
    "\n",
    "# Imputing with Mode as it is categorical\n",
    "application_without_ID_Target['CNT_FAM_MEMBERS'] = application_without_ID_Target['CNT_FAM_MEMBERS'].fillna(application_without_ID_Target['CNT_FAM_MEMBERS'].mode().iloc[0])\n",
    "\n",
    "print(\"Imputing and Encoding Done!!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:28.839704Z",
     "iopub.status.busy": "2022-12-13T18:14:28.839333Z",
     "iopub.status.idle": "2022-12-13T18:51:18.770680Z",
     "shell.execute_reply": "2022-12-13T18:51:18.769599Z",
     "shell.execute_reply.started": "2022-12-13T18:14:28.839660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing Numeriacal Varaible Started.....\n",
      "\n",
      "Mean Imputation Started....\n",
      "\n",
      "Saving the Mean Imputed File.....\n",
      "Saved!!!\n",
      "Mean Imputation Done!!!\n",
      "\n",
      "MICE Imputation Started....\n",
      "\n",
      "Saving the MICE imputed file....\n",
      "Saved!!!\n",
      "MICE Imputation Done!!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Imputing Numeriacal Varaible Started.....\\n\")\n",
    "# print(\"Mean Imputation Started....\\n\")\n",
    "\n",
    "# # Imputing using Mean Strategy on numerical Columns\n",
    "# application_mean = mean_imputation(application_without_ID_Target)\n",
    "\n",
    "# print(\"Saving the Mean Imputed File.....\")\n",
    "# # Saving the final application after imputation into a csv\n",
    "# application_mean['SK_ID_CURR'] = df['SK_ID_CURR']\n",
    "# application_mean['TARGET'] = df['TARGET']\n",
    "# application_mean.to_csv('/kaggle/working/application_mean.csv',index=False)\n",
    "\n",
    "application_mean = pd.read_csv(\"application_mean.csv\") # Here read the file\n",
    "X_mean = application_mean.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_mean = application_mean['TARGET']\n",
    "# print(\"Saved!!!\")\n",
    "\n",
    "# print(f\"Mean Imputation Done!!!\\n\")\n",
    "\n",
    "# print(f\"MICE Imputation Started....\\n\")\n",
    "# application_mice = mice_imputation(application_without_ID_Target)\n",
    "\n",
    "# print(\"Saving the MICE imputed file....\")\n",
    "# # Saving the final application after imputation into a csv\n",
    "# application_mice['SK_ID_CURR'] = df['SK_ID_CURR']\n",
    "# application_mice['TARGET'] = df['TARGET']\n",
    "# application_mice.to_csv('/kaggle/working/application_mice.csv',index=False)\n",
    "\n",
    "application_mice = pd.read_csv(\"application_mice.csv\") # Here read the file\n",
    "X_mice = application_mice.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_mice = application_mice['TARGET']\n",
    "# print(\"Saved!!!\")\n",
    "\n",
    "# print(f\"MICE Imputation Done!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Checking if imputation didnt change the distribution of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:51:18.774126Z",
     "iopub.status.busy": "2022-12-13T18:51:18.773785Z",
     "iopub.status.idle": "2022-12-13T18:51:18.778978Z",
     "shell.execute_reply": "2022-12-13T18:51:18.777659Z",
     "shell.execute_reply.started": "2022-12-13T18:51:18.774095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean Imputation result\n",
    "\n",
    "# MICE Imputation result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "    In this we would fir two model's for each category one would be the basic and another would be the tuned one. We will use Stratified KFold Cross Validation for that and as a performance metric we will use AUC(Area under Curve) ROC(Reciever Operating Characterstics) as it is highly imbalanced dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:05:36.086071Z",
     "iopub.status.busy": "2022-12-13T19:05:36.085663Z",
     "iopub.status.idle": "2022-12-13T19:05:36.094194Z",
     "shell.execute_reply": "2022-12-13T19:05:36.092995Z",
     "shell.execute_reply.started": "2022-12-13T19:05:36.086037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe to keep the track of all the results \n",
    "df_models = pd.DataFrame(columns = ['Model Name', 'Algorithm', 'Features', 'Target', 'Train AUC', 'Valid AUC', 'Test AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Imputed with no Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:05:38.788733Z",
     "iopub.status.busy": "2022-12-13T19:05:38.788303Z",
     "iopub.status.idle": "2022-12-13T19:06:26.130160Z",
     "shell.execute_reply": "2022-12-13T19:06:26.128784Z",
     "shell.execute_reply.started": "2022-12-13T19:05:38.788695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING MEAN IMPUTATION NO TUNING MODEL FITTING>>>\n",
      "\n",
      "Started KFold Cross Validation.....\n",
      "KFold Cross Validation Ended!!!!\n",
      "\n",
      "Total time taken to fit the model is 2.2266940514246625 min\n",
      "\n",
      "Starting predicting on the Test data....\n",
      "Prediction done on test data!!!!\n",
      "\n",
      "Appending the resulted Model in the Models Dataframe.....\n",
      "Model is added in the Models Dataframe!!!\n",
      "\n",
      "ENDED THE MEAN IMPUTATION NO TUNING MODEL FITTING!!!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean Imputed Model with no tuning\n",
    "print(f\"STARTING MEAN IMPUTATION NO TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mean, y_mean, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression()\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'Mean Imputed No Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mean.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MEAN IMPUTATION NO TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Imputed with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:06:26.133556Z",
     "iopub.status.busy": "2022-12-13T19:06:26.132761Z",
     "iopub.status.idle": "2022-12-13T19:06:26.140841Z",
     "shell.execute_reply": "2022-12-13T19:06:26.139428Z",
     "shell.execute_reply.started": "2022-12-13T19:06:26.133503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the tuned parameters\n",
    "# print(\"Finding the regularization parameter.....\")\n",
    "# c_score = tuning_C(X, y)\n",
    "# print(\"Finding the Solver.....\")\n",
    "# solver = tuning_solver(X_mice, y_mice)\n",
    "# print(\"Solver found!!!!\")\n",
    "# print(\"Finding the correct k-folds.....\")\n",
    "# n_splits = tuning_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:06:26.143209Z",
     "iopub.status.busy": "2022-12-13T19:06:26.142650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING MEAN IMPUTATION TUNING MODEL FITTING>>>\n",
      "\n",
      "Started KFold Cross Validation.....\n",
      "KFold Cross Validation Ended!!!!\n",
      "\n",
      "Total time taken to fit the model is 35.85205524365107 min\n",
      "\n",
      "Starting predicting on the Test data....\n"
     ]
    }
   ],
   "source": [
    "# Mean Imputed Model with tuning\n",
    "print(f\"STARTING MEAN IMPUTATION TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mean, y_mean, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression(C = 0.1, penalty = 'l1',random_state = 42, solver ='liblinear', max_iter= 3000, class_weight='balanced')\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'Mean Imputed With Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mean.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MEAN IMPUTATION WITH TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE Imputed with no tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mice Imputed Model with no tuning\n",
    "print(f\"STARTING MICE IMPUTATION WITH NO TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mice, y_mice, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression()\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'MICE Imputed No Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mice.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MICE IMPUTATION WITH NO TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE Imputed with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the tuned parameters\n",
    "# print(\"Finding the regularization parameter.....\")\n",
    "# c_score = tuning_C(X, y)\n",
    "# print(\"Finding the Solver.....\")\n",
    "# solver = tuning_solver(X_mice, y_mice)\n",
    "# print(\"Solver found!!!!\")\n",
    "# print(\"Finding the correct k-folds.....\")\n",
    "# n_splits = tuning_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mice Imputed Model with tuning\n",
    "print(f\"STARTING MICE IMPUTATION WITH TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mice, y_mice, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression(C = 0.1, penalty = 'l1',random_state = 42, solver ='liblinear', max_iter= 3000, class_weight='balanced')\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'MICE Imputed with Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mice.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MICE IMPUTATION WITH TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection in Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using IV\n",
    "print(\"IV Calculation begin.....\")\n",
    "iv_df = IV_calculation(X_mice, y_mice, 20)\n",
    "\n",
    "print(\"IV Calculation Done!!!!\")\n",
    "## Feature selected from IV are those between 0.03 to 0.1 are evaluated \n",
    "## and greater than 0.1 is Retained\n",
    "iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all other columns\n",
    "application = application_mice[['EXT_SOURCE_3','EXT_SOURCE_2','DAYS_EMPLOYED','DAYS_BIRTH','AMT_GOODS_PRICE','AMT_CREDIT', \n",
    "                                'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE','DAYS_REGISTRATION', 'REGION_RATING_CLIENT', \n",
    "                                'REGION_RATING_CLIENT_W_CITY', 'TARGET','SK_ID_CURR',\n",
    "                                'NAME_EDUCATION_TYPE_Higher education']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the other datasets and Merging\n",
    "    Importing the other files and merging with the reduced application as we will try to increase the Models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Shape\n",
    "print(f'Initial Train Shape:{application.shape}')\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Combining Dataframes to Train\n",
    "def merge_train(df):\n",
    "    train_final = pd.merge(application, df, how='left', on = ['SK_ID_CURR'])\n",
    "    return train_final\n",
    "\n",
    "train = merge_train(bureau)\n",
    "print(f'New shape after bureau/bureau balance: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "train = merge_train(POS_CASH_balance)\n",
    "print(f'New shape after POS_CASH_balance: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "train = merge_train(installments_payments)\n",
    "print(f'New shape after installments_payments: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "train = merge_train(credit_card_balance)\n",
    "print(f'New shape after credit_card_balance: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "final_df = merge_train(previous_application)\n",
    "print(f'New shape after previous_application: {final_df.shape}')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = dropping_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['SK_ID_CURR', 'TARGET', 'SK_ID_PREV'], axis = 1)\n",
    "y = final_df['TARGET']\n",
    "\n",
    "\n",
    "iv_df = IV_calculation(X, y, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['EXT_SOURCE_3', 'EXT_SOURCE_2', 'DAYS_EMPLOYED', 'DAYS_BIRTH','REGION_RATING_CLIENT_W_CITY', \n",
    "                    'PRODUCT_COMBINATION', 'REGION_RATING_CLIENT', 'CODE_REJECT_REASON', 'DAYS_LAST_PHONE_CHANGE','NAME_CONTRACT_STATUS',\n",
    "                    'NAME_EDUCATION_TYPE_Higher education', 'DAYS_ID_PUBLISH', 'AMT_GOODS_PRICE_x', 'AMT_CREDIT_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = imputing_categorical_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encoding_categorical(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.1, penalty='l1',class_weight = 'balanced', random_state=42, solver = 'liblinear')\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc', return_train_score = True)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training ROC\": np.mean(cv_scores['train_score']))\n",
    "print(f\"Validating ROC: {np.mean(cv_scores['test_score'])}\")\n",
    "print(f\"Test ROC: {roc_auc_score(y_test, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
